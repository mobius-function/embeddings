{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Generation Inference\n",
    "\n",
    "This notebook demonstrates how to use the trained face generation model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.encoder import get_encoder\n",
    "from src.generator import get_generator\n",
    "from src.utils import show_tensor_images, load_image, interpolate_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EMBEDDING_SIZE = 256\n",
    "Z_DIM = 512\n",
    "IMG_SIZE = 128\n",
    "CHECKPOINT_PATH = '../output/checkpoints/checkpoint_epoch_100.pth'  # Update with your checkpoint path\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TEST_DIR = '../data/test'  # Path to test images\n",
    "\n",
    "# Load models\n",
    "encoder = get_encoder(embedding_size=EMBEDDING_SIZE, device=DEVICE)\n",
    "generator = get_generator(\n",
    "    z_dim=Z_DIM, \n",
    "    embedding_size=EMBEDDING_SIZE, \n",
    "    img_size=IMG_SIZE, \n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Load checkpoint if exists\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    generator.load_state_dict(checkpoint['generator'])\n",
    "    print(f\"Loaded checkpoint from epoch {checkpoint['epoch']+1}\")\n",
    "else:\n",
    "    print(f\"No checkpoint found at {CHECKPOINT_PATH}\")\n",
    "    \n",
    "# Set models to evaluation mode\n",
    "encoder.eval()\n",
    "generator.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Faces from Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some test images\n",
    "test_image_paths = glob.glob(os.path.join(TEST_DIR, '*'))[:16]  # Adjust number as needed\n",
    "\n",
    "# Load images\n",
    "test_images = []\n",
    "for path in test_image_paths:\n",
    "    img = load_image(path, img_size=IMG_SIZE).to(DEVICE)\n",
    "    test_images.append(img)\n",
    "    \n",
    "test_images = torch.cat(test_images, dim=0)\n",
    "\n",
    "# Display original test images\n",
    "print(\"Original Test Images:\")\n",
    "show_tensor_images(test_images, num_images=16, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reconstructions\n",
    "with torch.no_grad():\n",
    "    # Extract embeddings\n",
    "    embeddings = encoder(test_images)\n",
    "    \n",
    "    # Sample noise vectors\n",
    "    z = torch.randn(embeddings.shape[0], Z_DIM).to(DEVICE)\n",
    "    \n",
    "    # Generate faces from embeddings\n",
    "    generated_images = generator(z, embeddings)\n",
    "    \n",
    "# Display reconstructions\n",
    "print(\"Generated Reconstructions:\")\n",
    "show_tensor_images(generated_images, num_images=16, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Style Mixing - Different Noise Vectors with Same Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single test image\n",
    "test_image = test_images[0].unsqueeze(0)\n",
    "\n",
    "# Generate multiple variations with different noise vectors\n",
    "num_variations = 8\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Extract embedding\n",
    "    embedding = encoder(test_image)\n",
    "    \n",
    "    # Repeat embedding\n",
    "    embedding = embedding.repeat(num_variations, 1)\n",
    "    \n",
    "    # Generate different noise vectors\n",
    "    z_vectors = torch.randn(num_variations, Z_DIM).to(DEVICE)\n",
    "    \n",
    "    # Generate faces\n",
    "    style_mixed_images = generator(z_vectors, embedding)\n",
    "\n",
    "# Show original image\n",
    "print(\"Original Image:\")\n",
    "show_tensor_images(test_image, num_images=1)\n",
    "\n",
    "# Show generated variations\n",
    "print(\"Same Identity with Different Styles:\")\n",
    "show_tensor_images(style_mixed_images, num_images=num_variations, figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interpolate Between Face Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two test images for interpolation\n",
    "image1 = test_images[0].unsqueeze(0)\n",
    "image2 = test_images[1].unsqueeze(0)\n",
    "\n",
    "# Display the selected images\n",
    "print(\"Image 1 and Image 2 for Interpolation:\")\n",
    "show_tensor_images(torch.cat([image1, image2], dim=0), num_images=2)\n",
    "\n",
    "# Extract embeddings\n",
    "with torch.no_grad():\n",
    "    embedding1 = encoder(image1)\n",
    "    embedding2 = encoder(image2)\n",
    "    \n",
    "    # Create interpolated embeddings\n",
    "    interpolated_embeddings = interpolate_embeddings(embedding1, embedding2, steps=8)\n",
    "    \n",
    "    # Use same noise vector for all interpolated images\n",
    "    z = torch.randn(1, Z_DIM).to(DEVICE)\n",
    "    z = z.repeat(interpolated_embeddings.shape[0], 1)\n",
    "    \n",
    "    # Generate images from interpolated embeddings\n",
    "    interpolated_images = generator(z, interpolated_embeddings)\n",
    "\n",
    "# Display interpolation results\n",
    "print(\"Interpolation Between Two Faces:\")\n",
    "show_tensor_images(interpolated_images, num_images=8, figsize=(15, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Faces from Random Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random embeddings\n",
    "num_samples = 16\n",
    "random_embeddings = torch.randn(num_samples, EMBEDDING_SIZE).to(DEVICE)\n",
    "z = torch.randn(num_samples, Z_DIM).to(DEVICE)\n",
    "\n",
    "# Generate faces from random embeddings\n",
    "with torch.no_grad():\n",
    "    random_faces = generator(z, random_embeddings)\n",
    "    \n",
    "# Display random generated faces\n",
    "print(\"Faces Generated from Random Embeddings:\")\n",
    "show_tensor_images(random_faces, num_images=num_samples, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Face Manipulation - Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate several faces with the same noise but different embeddings\n",
    "num_faces = 100\n",
    "z = torch.randn(1, Z_DIM).to(DEVICE)\n",
    "z = z.repeat(num_faces, 1)\n",
    "random_embeddings = torch.randn(num_faces, EMBEDDING_SIZE).to(DEVICE)\n",
    "\n",
    "# Generate faces\n",
    "with torch.no_grad():\n",
    "    faces = generator(z, random_embeddings)\n",
    "    \n",
    "    # Compute mean embedding\n",
    "    mean_embedding = random_embeddings.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    # Generate face with mean embedding\n",
    "    mean_face = generator(z[:1], mean_embedding)\n",
    "    \n",
    "# Display mean face\n",
    "print(\"Mean Face:\")\n",
    "show_tensor_images(mean_face, num_images=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some sample faces\n",
    "sample_faces = faces[:8]\n",
    "\n",
    "# Display sample faces\n",
    "print(\"Sample Generated Faces:\")\n",
    "show_tensor_images(sample_faces, num_images=8, figsize=(15, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component manipulation (like PCA but simpler)\n",
    "# Pick a random direction in embedding space\n",
    "direction = torch.randn(EMBEDDING_SIZE).to(DEVICE)\n",
    "direction = direction / direction.norm()  # Normalize\n",
    "\n",
    "# Create variations by moving in that direction\n",
    "variations = []\n",
    "strengths = [-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for strength in strengths:\n",
    "        # Modify the mean embedding\n",
    "        modified_embedding = mean_embedding + direction * strength\n",
    "        \n",
    "        # Generate face\n",
    "        modified_face = generator(z[:1], modified_embedding)\n",
    "        variations.append(modified_face)\n",
    "        \n",
    "    variations = torch.cat(variations, dim=0)\n",
    "\n",
    "# Display variations\n",
    "print(\"Manipulating Face Attributes:\")\n",
    "show_tensor_images(variations, num_images=len(strengths), figsize=(15, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Interpolation Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install imageio if not already installed\n",
    "# !pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def tensor_to_numpy(img_tensor):\n",
    "    \"\"\"Convert a tensor image to numpy image.\"\"\"\n",
    "    # Convert to CPU, detach from computation graph, and convert to numpy\n",
    "    img = img_tensor.cpu().detach().numpy()\n",
    "    # Change from CxHxW to HxWxC\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    # Convert from [-1, 1] to [0, 255]\n",
    "    img = (img + 1) / 2\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def create_interpolation_video(filename, image1, image2, steps=60, fps=30):\n",
    "    \"\"\"Create a smooth interpolation video between two images.\"\"\"\n",
    "    # Extract embeddings\n",
    "    with torch.no_grad():\n",
    "        embedding1 = encoder(image1)\n",
    "        embedding2 = encoder(image2)\n",
    "        \n",
    "        # Create interpolated embeddings\n",
    "        interpolated_embeddings = interpolate_embeddings(embedding1, embedding2, steps=steps)\n",
    "        \n",
    "        # Use same noise vector for all interpolated images\n",
    "        z = torch.randn(1, Z_DIM).to(DEVICE)\n",
    "        z = z.repeat(interpolated_embeddings.shape[0], 1)\n",
    "        \n",
    "        # Generate images from interpolated embeddings\n",
    "        frames = []\n",
    "        for i in tqdm(range(interpolated_embeddings.shape[0])):\n",
    "            emb = interpolated_embeddings[i:i+1]\n",
    "            z_i = z[i:i+1]\n",
    "            gen_img = generator(z_i, emb)\n",
    "            \n",
    "            # Convert to numpy\n",
    "            frame = tensor_to_numpy(gen_img[0])\n",
    "            frames.append(frame)\n",
    "        \n",
    "        # Create forward and backward loop\n",
    "        frames_loop = frames + frames[::-1]\n",
    "        \n",
    "        # Save video\n",
    "        imageio.mimsave(filename, frames_loop, fps=fps)\n",
    "        \n",
    "        print(f\"Video saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two images for interpolation\n",
    "image1 = test_images[0].unsqueeze(0)\n",
    "image2 = test_images[3].unsqueeze(0)\n",
    "\n",
    "# Display the selected images\n",
    "print(\"Image 1 and Image 2 for Video Interpolation:\")\n",
    "show_tensor_images(torch.cat([image1, image2], dim=0), num_images=2)\n",
    "\n",
    "# Create interpolation video\n",
    "output_video = \"../output/face_interpolation.mp4\"\n",
    "os.makedirs(os.path.dirname(output_video), exist_ok=True)\n",
    "create_interpolation_video(output_video, image1, image2, steps=30, fps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Multiple Styles for a Single Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_style_grid(reference_image, num_styles=16, figsize=(12, 8)):\n",
    "    \"\"\"Generate multiple style variations of a single face.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Extract embedding\n",
    "        embedding = encoder(reference_image)\n",
    "        \n",
    "        # Repeat embedding\n",
    "        embedding = embedding.repeat(num_styles, 1)\n",
    "        \n",
    "        # Generate different noise vectors\n",
    "        z_vectors = torch.randn(num_styles, Z_DIM).to(DEVICE)\n",
    "        \n",
    "        # Generate faces\n",
    "        style_variations = generator(z_vectors, embedding)\n",
    "    \n",
    "    # Display reference image\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.title(\"Reference Image\")\n",
    "    show_tensor_images(reference_image, num_images=1)\n",
    "    \n",
    "    # Display style variations\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(\"Style Variations\")\n",
    "    show_tensor_images(style_variations, num_images=num_styles)\n",
    "    \n",
    "    return style_variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a reference image\n",
    "reference_image = test_images[5].unsqueeze(0)\n",
    "\n",
    "# Generate style variations\n",
    "variations = generate_style_grid(reference_image, num_styles=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
